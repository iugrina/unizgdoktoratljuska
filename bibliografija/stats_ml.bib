
@article{hancock_lower_1996,
	title = {Lower Bounds on Learning Decision Lists and Trees},
	volume = {126},
	issn = {0890-5401},
	url = {http://www.sciencedirect.com/science/article/pii/S0890540196900401},
	doi = {10.1006/inco.1996.0040},
	abstract = {k-Decision lists and decision trees play important roles in learning theory as well as in practical learning systems.k-Decision lists generalize classes such as monomials,k-{DNF}, andk-{CNF}, and like these subclasses they are polynomially {PAC-learnable} [R. {Rivest,Mach.} Learning2(1987), 229–246]. This leaves open the question of whetherk-decision lists can be learned as efficiently ask-{DNF.} We answer this question negatively in a certain sense, thus disproving a claim in a popular textbook [M. Anthony and N. Biggs, {“Computational} Learning Theory,” Cambridge Univ. Press, Cambridge, {UK}, 1992]. Decision trees, on the other hand, are not even known to be polynomially {PAC-learnable}, despite their widespread practical application. We will show that decision trees are not likely to be efficiently {PAC-learnable.} We summarize our specific results. The following problems cannot be approximated in polynomial time within a factor of 2logδ nfor anyδ\&lt;1, {unlessNP⊂DTIME[2polylog} n]: a generalized set cover,k-decision lists,k-decision lists by monotone decision lists, and decision trees. Decision lists cannot be approximated in polynomial time within a factor ofnδ, for some constantδ\&gt;0, {unlessNP=P.} Also,k-decision lists withl0–1 alternations cannot be approximated within a factor logl {nunlessNP⊂DTIME[nO(log} log n)] (providing an interesting comparison to the upper bound obtained by A. Dhagat and L. Hellerstein [{in“FOCS} '94,” pp. 64–74]).},
	number = {2},
	urldate = {2013-11-29},
	journal = {Information and Computation},
	author = {Hancock, Thomas and Jiang, Tao and Li, Ming and Tromp, John},
	month = may,
	year = {1996},
	keywords = {approximation algorithm, computational complexity, decision list, decision tree, {PAC-learning}},
	pages = {114--122},
	file = {ScienceDirect Full Text PDF:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/GQE8C6N3/Hancock et al. - 1996 - Lower Bounds on Learning Decision Lists and Trees.pdf:application/pdf;ScienceDirect Snapshot:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/P4Z9IE4V/S0890540196900401.html:text/html}
},

@article{hanley_meaning_1982,
	title = {The meaning and use of the area under a receiver operating characteristic ({ROC)} curve.},
	volume = {143},
	issn = {0033-8419, 1527-1315},
	url = {http://radiology.rsna.org/content/143/1/29},
	language = {en},
	number = {1},
	urldate = {2013-10-14},
	journal = {Radiology},
	author = {Hanley, J. A. and {McNeil}, B. J.},
	month = apr,
	year = {1982},
	note = {{PMID:} 7063747},
	pages = {29--36},
	file = {Full Text PDF:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/RBXVQDJX/Hanley and McNeil - 1982 - The meaning and use of the area under a receiver o.pdf:application/pdf;Snapshot:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/MGEF9S42/29.html:text/html}
},

@book{breiman_classification_1984,
	title = {Classification and regression trees},
	isbn = {9780412048418},
	abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
	language = {en},
	publisher = {Chapman \& Hall},
	author = {Breiman, Leo},
	month = jan,
	year = {1984},
	keywords = {Business \& Economics / Statistics, Discriminant analysis, Mathematics / Graphic Methods, Mathematics / Probability \& Statistics / General, Nature / Plants / Trees, Regression analysis, Trees (Graph theory)}
},

@article{fawcett_introduction_2006,
	title = {An introduction to {ROC} analysis},
	volume = {27},
	issn = {0167-8655},
	url = {http://dx.doi.org/10.1016/j.patrec.2005.10.010},
	doi = {10.1016/j.patrec.2005.10.010},
	abstract = {Receiver operating characteristics ({ROC)} graphs are useful for organizing classifiers and visualizing their performance. {ROC} graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although {ROC} graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to {ROC} graphs and as a guide for using them in research.},
	number = {8},
	urldate = {2013-10-14},
	journal = {Pattern Recogn. Lett.},
	author = {Fawcett, Tom},
	month = jun,
	year = {2006},
	keywords = {Classifier evaluation, Evaluation metrics, {ROC} analysis},
	pages = {861–874},
	file = {ROCintro.pdf:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/XPXEDZTT/ROCintro.pdf:application/pdf}
},

@article{powers_evaluation:_2011,
	title = {Evaluation: From Precision, Recall and F-Measure to {ROC.}, Informedness, Markedness \& Correlation},
	volume = {2},
	lccn = {+++},
	shorttitle = {Evaluation},
	url = {http://www.bioinfo.in/uploadfiles/13031311552_1_1_JMLT.pdf},
	number = {1},
	urldate = {2012-10-29},
	journal = {Journal of Machine Learning Technologies},
	author = {Powers, D. M. W.},
	year = {2011},
	note = {Cited by 0086},
	keywords = {{AUC}, Metrics, {ROC}},
	pages = {37–63},
	file = {13031311552_1_1_JMLT.pdf:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/H9CBGXI5/13031311552_1_1_JMLT.pdf:application/pdf}
},

@article{efron_leisurely_1983,
	title = {A Leisurely Look at the Bootstrap, the Jackknife, and Cross-Validation},
	volume = {37},
	copyright = {Copyright © 1983 American Statistical Association},
	issn = {0003-1305},
	url = {http://www.jstor.org/stable/2685844},
	doi = {10.2307/2685844},
	abstract = {This is an invited expository article for The American Statistician. It reviews the nonparametric estimation of statistical error, mainly the bias and standard error of an estimator, or the error rate of a prediction rule. The presentation is written at a relaxed mathematical level, omitting most proofs, regularity conditions, and technical details.},
	number = {1},
	urldate = {2013-12-15},
	journal = {The American Statistician},
	author = {Efron, Bradley and Gong, Gail},
	month = feb,
	year = {1983},
	pages = {36--48},
	file = {JSTOR Full Text PDF:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/W3749ZJ5/Efron and Gong - 1983 - A Leisurely Look at the Bootstrap, the Jackknife, .pdf:application/pdf}
},

@article{heagerty_survival_2005,
	title = {Survival Model Predictive Accuracy and {ROC} Curves},
	volume = {61},
	copyright = {Copyright © 2005 International Biometric Society},
	issn = {0006-{341X}},
	url = {http://www.jstor.org/stable/3695651},
	doi = {10.2307/3695651},
	abstract = {The predictive accuracy of a survival model can be summarized using extensions of the proportion of variation explained by the model, or R2, commonly used for continuous response models, or using extensions of sensitivity and specificity, which are commonly used for binary response models. In this article we propose new time-dependent accuracy summaries based on time-specific versions of sensitivity and specificity calculated over risk sets. We connect the accuracy summaries to a previously proposed global concordance measure, which is a variant of Kendall's tau. In addition, we show how standard Cox regression output can be used to obtain estimates of time-dependent sensitivity and specificity, and time-dependent receiver operating characteristic ({ROC)} curves. Semiparametric estimation methods appropriate for both proportional and nonproportional hazards data are introduced, evaluated in simulations, and illustrated using two familiar survival data sets.},
	number = {1},
	urldate = {2013-10-14},
	journal = {Biometrics},
	author = {Heagerty, Patrick J. and Zheng, Yingye},
	month = mar,
	year = {2005},
	note = {{ArticleType:} research-article / Full publication date: Mar., 2005 / Copyright © 2005 International Biometric Society},
	pages = {92--105},
	file = {JSTOR Full Text PDF:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/F3ZSUB2P/Heagerty and Zheng - 2005 - Survival Model Predictive Accuracy and ROC Curves.pdf:application/pdf}
},

@article{heagerty_time-dependent_2000,
	title = {Time-Dependent {ROC} Curves for Censored Survival Data and a Diagnostic Marker},
	volume = {56},
	copyright = {Copyright © 2000 International Biometric Society},
	issn = {0006-{341X}},
	url = {http://www.jstor.org/stable/2676971},
	doi = {10.2307/2676971},
	abstract = {{ROC} curves are a popular method for displaying sensitivity and specificity of a continuous diagnostic marker, X, for a binary disease variable, D. However, many disease outcomes are time dependent, D(t), and {ROC} curves that vary as a function of time may be more appropriate. A common example of a time-dependent variable is vital status, where D(t) = 1 if a patient has died prior to time t and zero otherwise. We propose summarizing the discrimination potential of a marker X, measured at baseline (t = 0), by calculating {ROC} curves for cumulative disease or death incidence by time t, which we denote as {ROC(t).} A typical complexity with survival data is that observations may be censored. Two {ROC} curve estimators are proposed that can accommodate censored data. A simple estimator is based on using the Kaplan-Meier estimator for each possible subset X {\textgreater} c. However, this estimator does not guarantee the necessary condition that sensitivity and specificity are monotone in X. An alternative estimator that does guarantee monotonicity is based on a nearest neighbor estimator for the bivariate distribution function of (X, T), where T represents survival time (Akritas, M. J., 1994, Annals of Statistics 22, 1299-1327). We present an example where {ROC(t)} is used to compare a standard and a modified flow cytometry measurement for predicting survival after detection of breast cancer and an example where the {ROC(t)} curve displays the impact of modifying eligibility criteria for sample size and power in {HIV} prevention trials.},
	number = {2},
	urldate = {2013-10-14},
	journal = {Biometrics},
	author = {Heagerty, Patrick J. and Lumley, Thomas and Pepe, Margaret S.},
	month = jun,
	year = {2000},
	note = {{ArticleType:} research-article / Full publication date: Jun., 2000 / Copyright © 2000 International Biometric Society},
	pages = {337--344},
	file = {JSTOR Full Text PDF:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/NSG6GDVV/Heagerty et al. - 2000 - Time-Dependent ROC Curves for Censored Survival Da.pdf:application/pdf}
},

@article{hyafil_constructing_1976,
	title = {Constructing optimal binary decision trees is {NP-complete}},
	volume = {5},
	issn = {0020-0190},
	url = {http://www.sciencedirect.com/science/article/pii/0020019076900958},
	doi = {10.1016/0020-0190(76)90095-8},
	number = {1},
	urldate = {2013-11-29},
	journal = {Information Processing Letters},
	author = {Hyafil, Laurent and Rivest, Ronald L.},
	month = may,
	year = {1976},
	keywords = {Binary decision trees, computational complexity, {NP-complete}},
	pages = {15--17},
	file = {ScienceDirect Snapshot:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/7UI95GZ5/0020019076900958.html:text/html}
},

@article{quinlan_induction_1986,
	title = {Induction of decision trees},
	volume = {1},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1007/BF00116251},
	doi = {10.1007/BF00116251},
	abstract = {The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, {ID3}, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.},
	language = {en},
	number = {1},
	urldate = {2013-11-29},
	journal = {Machine Learning},
	author = {Quinlan, J. R.},
	month = mar,
	year = {1986},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, classification, Computing Methodologies, decision trees, expert systems, induction, information theory, knowledge acquisition, Language Translation and Linguistics, Simulation and Modeling},
	pages = {81--106},
	file = {Full Text PDF:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/4FVRNEA3/Quinlan - 1986 - Induction of decision trees.pdf:application/pdf;Snapshot:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/6S8KQM3M/10.html:text/html}
},

@article{salzberg_c4.5:_1994,
	title = {C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993},
	volume = {16},
	issn = {0885-6125, 1573-0565},
	shorttitle = {C4.5},
	url = {http://link.springer.com/article/10.1007/BF00993309},
	doi = {10.1007/BF00993309},
	language = {en},
	number = {3},
	urldate = {2013-11-29},
	journal = {Machine Learning},
	author = {Salzberg, Steven L.},
	month = sep,
	year = {1994},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, Computing Methodologies, Language Translation and Linguistics, Simulation and Modeling},
	pages = {235--240},
	file = {Full Text PDF:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/8D8TNFRD/Salzberg - 1994 - C4.5 Programs for Machine Learning by J. Ross Qui.pdf:application/pdf;Snapshot:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/FQERWNP9/10.html:text/html}
},

@techreport{mitchell_need_1980,
	title = {The Need for Biases in Learning Generalizations},
	language = {en},
	institution = {Department of Computer Science, Laboratory for Computer Science Research, Rutgers Univ.},
	author = {Mitchell, Tom M.},
	year = {1980},
	pages = {7}
},

@article{wolpert_lack_1996,
	title = {The Lack of A Priori Distinctions Between Learning Algorithms},
	volume = {8},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/neco.1996.8.7.1341},
	doi = {10.1162/neco.1996.8.7.1341},
	abstract = {This is the first of two papers that use off-training set ({OTS)} error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no a priori distinctions between learning algorithms. (The second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected {OTS} error than B as vice versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one cannot say: if empirical misclassification rate is low, the Vapnik-Chervonenkis dimension of your generalizer is small, and the training set is large, then with high probability your {OTS} error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.},
	number = {7},
	urldate = {2013-11-30},
	journal = {Neural Computation},
	author = {Wolpert, David H.},
	month = oct,
	year = {1996},
	pages = {1341--1390},
	file = {Neural Computation Snapshot:/home/iugrina/.mozilla/firefox/u5x6ox3v.znanost/zotero/storage/Q82KV4U2/neco.1996.8.7.html:text/html}
},

@book{rokach_data_2008,
	series = {Series in Machine Perception and Artificial Intelligence},
	title = {Data Mining with Decision Trees: Theory and Applications},
	volume = {69},
	isbn = {9789812771711},
	shorttitle = {Data Mining with Decision Trees},
	language = {en},
	publisher = {World Scientific},
	author = {Rokach, Lior and Maimon, Oded},
	year = {2008},
	keywords = {Business \& Economics / Business Ethics, Computers / Databases / Data Mining, Computers / Machine Theory, Computers / Management Information Systems}
},

@article{chang_libsvm:_2011,
	title = {{LIBSVM:} A library for support vector machines},
	volume = {2},
	issn = {2157-6904},
	shorttitle = {{LIBSVM}},
	url = {http://dx.doi.org/10.1145/1961189.1961199},
	doi = {10.1145/1961189.1961199},
	abstract = {{LIBSVM} is a library for Support Vector Machines ({SVMs).} We have been actively developing this package since the year 2000. The goal is to help users to easily apply {SVM} to their applications. {LIBSVM} has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of {LIBSVM.} Issues such as solving {SVM} optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.},
	number = {3},
	urldate = {2013-12-15},
	journal = {{ACM} Trans. Intell. Syst. Technol.},
	author = {Chang, Chih and Lin, Chih},
	year = {2011},
	keywords = {learning, software-library, support-vector-machines}
},

@article{karatzoglou_support_2006,
	title = {Support Vector Machines in R},
	volume = {15},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v15/i09},
	number = {9},
	journal = {Journal of Statistical Software},
	author = {Karatzoglou, Alexandros and Meyer, David and Hornik, Kurt},
	year = {2006},
	pages = {1–28}
},

@book{jolliffe_principal_2002,
	title = {Principal Component Analysis},
	isbn = {9780387954424},
	abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
	language = {en},
	publisher = {Springer},
	author = {Jolliffe, I. T.},
	month = oct,
	year = {2002},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Multivariate Analysis, Mathematics / Probability \& Statistics / Stochastic Processes}
}